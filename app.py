import streamlit as st
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Set page configuration
st.set_page_config(
    page_title="Marmara Akademik Chatbot",
    page_icon="ğŸ“",
    layout="wide"
)

st.title("ğŸ“ Marmara Akademik Chatbot")
st.write("Bu chatbot, Marmara Ãœniversitesi akademik dokÃ¼manlarÄ± hakkÄ±nda sorularÄ±nÄ±zÄ± yanÄ±tlamak iÃ§in geliÅŸtirilmiÅŸtir.")

# Sidebar for model selection
st.sidebar.title("Model SeÃ§imi")
model_option = st.sidebar.radio(
    "Kullanmak istediÄŸiniz modeli seÃ§in:",
    ["Gemini", "OpenAI"]
)

# Function to load and process the document
@st.cache_resource
def load_and_process_document():
    # Load the PDF document
    loader = PyPDFLoader("data/marmara_yonerge.pdf")
    data = loader.load()
    
    # Split the document into chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = text_splitter.split_documents(data)
    
    return docs

# Function to create vector store based on model selection
@st.cache_resource
def create_vector_store(_docs, model_type):
    if model_type == "Gemini":
        # Check if GOOGLE_API_KEY is set
        if not os.getenv("GOOGLE_API_KEY"):
            st.error("GOOGLE_API_KEY bulunamadÄ±! LÃ¼tfen .env dosyasÄ±nda API anahtarÄ±nÄ±zÄ± tanÄ±mlayÄ±n.")
            return None
        
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        persist_directory = "./chroma_db_gemini"
    else:  # OpenAI
        # Check if OPENAI_API_KEY is set
        if not os.getenv("OPENAI_API_KEY"):
            st.error("OPENAI_API_KEY bulunamadÄ±! LÃ¼tfen .env dosyasÄ±nda API anahtarÄ±nÄ±zÄ± tanÄ±mlayÄ±n.")
            return None
        
        embeddings = OpenAIEmbeddings()
        persist_directory = "./chroma_db_openai"
    
    # Create or load the vector store
    try:
        vectorstore = Chroma.from_documents(
            documents=_docs,
            embedding=embeddings,
            persist_directory=persist_directory
        )
        return vectorstore
    except Exception as e:
        st.error(f"VektÃ¶r veri tabanÄ± oluÅŸturulurken hata: {str(e)}")
        return None

# Function to create and run the RAG chain
def run_rag_chain(query, model_type, vectorstore):
    if vectorstore is None:
        return "VektÃ¶r veri tabanÄ± oluÅŸturulamadÄ±."
    
    # Create a retriever from the vector store
    retriever = vectorstore.as_retriever(
        search_type="similarity", 
        search_kwargs={"k": 5}
    )
    
    # Create the language model based on the selected option
    if model_type == "Gemini":
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-pro",
            temperature=0.3,
            max_tokens=800
        )
    else:  # OpenAI
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.3,
            max_tokens=800
        )
    
    # Create a prompt template
    system_prompt = (
        "Sen Marmara Ãœniversitesi hakkÄ±nda bilgi veren bir asistansÄ±n. "
        "AÅŸaÄŸÄ±da verilen dÃ¶kÃ¼man parÃ§alarÄ±nÄ± kullanarak kullanÄ±cÄ±nÄ±n sorusunu yanÄ±tla. "
        "EÄŸer cevabÄ± bilmiyorsan, bilmediÄŸini sÃ¶yle ve tahmin etme. "
        "CevabÄ±nÄ± TÃ¼rkÃ§e olarak ver ve mÃ¼mkÃ¼n olduÄŸunca kÄ±sa ve Ã¶z tut. "
        "AyrÄ±ca, cevap verirken akademik ve resmi bir dil kullan. "
        "\n\n"
        "Verilen dÃ¶kÃ¼man parÃ§alarÄ±: {context}"
    )
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])
    
    # Create the question answering chain
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    
    # Create the retrieval chain
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)
    
    # Run the chain
    try:
        response = rag_chain.invoke({"input": query})
        return response["answer"]
    except Exception as e:
        return f"Hata oluÅŸtu: {str(e)}"

# Display proper messages for API keys
if model_option == "Gemini" and not os.getenv("GOOGLE_API_KEY"):
    st.warning("Gemini API anahtarÄ± bulunamadÄ±. LÃ¼tfen .env dosyasÄ±nda GOOGLE_API_KEY tanÄ±mlayÄ±n.")

if model_option == "OpenAI" and not os.getenv("OPENAI_API_KEY"):
    st.warning("OpenAI API anahtarÄ± bulunamadÄ±. LÃ¼tfen .env dosyasÄ±nda OPENAI_API_KEY tanÄ±mlayÄ±n.")

# Process the document
docs = load_and_process_document()

# Load chat history from session state
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Get user input
query = st.chat_input("Marmara Ãœniversitesi hakkÄ±nda bir soru sorun...")

# When the user submits a query
if query:
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": query})
    
    # Display user message
    with st.chat_message("user"):
        st.markdown(query)
    
    # Display assistant response in chat message container
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("DÃ¼ÅŸÃ¼nÃ¼yorum...")
        
        # Create vector store based on model selection
        vectorstore = create_vector_store(docs, model_option)
        
        # Run the RAG chain
        if vectorstore:
            response = run_rag_chain(query, model_option, vectorstore)
            message_placeholder.markdown(response)
            
            # Add assistant response to chat history
            st.session_state.messages.append({"role": "assistant", "content": response})
        else:
            message_placeholder.markdown("API anahtarÄ± eksik olduÄŸu iÃ§in yanÄ±t Ã¼retilemedi.")

# Display information about the project
with st.expander("Proje HakkÄ±nda"):
    st.write("""
    Bu chatbot, Marmara Ãœniversitesi akademik dokÃ¼manlarÄ± Ã¼zerinde Retrieval Augmented Generation (RAG) 
    teknolojisi kullanÄ±larak geliÅŸtirilmiÅŸtir. Gemini ve OpenAI LLM modellerini kullanarak sorularÄ±nÄ±zÄ± yanÄ±tlar.
    
    **NasÄ±l Ã‡alÄ±ÅŸÄ±r?**
    1. PDF dokÃ¼manlarÄ± okunur ve iÅŸlenir
    2. Metin parÃ§alarÄ±na ayrÄ±lÄ±r ve vektÃ¶r veritabanÄ±na kaydedilir
    3. Soru sorduÄŸunuzda en ilgili metin parÃ§alarÄ± bulunur
    4. SeÃ§ilen LLM modeli bu parÃ§alarÄ± kullanarak sorunuzu yanÄ±tlar
    
    **KullanÄ±lan Teknolojiler:**
    - LangChain
    - ChromaDB
    - Streamlit
    - Google Gemini / OpenAI
    """) 